{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e050ba73-1375-4d8d-9d65-68b7a50c1527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-storage-file-datalake\n  Downloading azure_storage_file_datalake-12.18.0-py3-none-any.whl (258 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.4/258.4 kB 6.0 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=4.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-83984164-bbbc-403b-9561-500bfdd5dc68/lib/python3.10/site-packages (from azure-storage-file-datalake) (4.12.2)\nCollecting azure-core>=1.30.0\n  Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198.9/198.9 kB 24.9 MB/s eta 0:00:00\nCollecting isodate>=0.6.1\n  Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\nCollecting azure-storage-blob>=12.24.0\n  Downloading azure_storage_blob-12.24.0-py3-none-any.whl (408 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 408.6/408.6 kB 41.1 MB/s eta 0:00:00\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.30.0->azure-storage-file-datalake) (1.16.0)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.10/site-packages (from azure-core>=1.30.0->azure-storage-file-datalake) (2.28.1)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob>=12.24.0->azure-storage-file-datalake) (39.0.1)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.24.0->azure-storage-file-datalake) (1.15.1)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-file-datalake) (2.0.4)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.24.0->azure-storage-file-datalake) (2.21)\nInstalling collected packages: isodate, azure-core, azure-storage-blob, azure-storage-file-datalake\nSuccessfully installed azure-core-1.32.0 azure-storage-blob-12.24.0 azure-storage-file-datalake-12.18.0 isodate-0.7.2\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c7f7f7e-2201-42ed-8b5f-c4d24d2c4a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Define the base URL for Ergast API\n",
    "base_url = \"https://ergast.com/api/f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eee2f82f-334c-4503-8a6b-cc56e39582d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbcUrl = \"jdbc:sqlserver://az-tfo-serv-001.database.windows.net:1433;database=az-tfo-dbs-001\"\n",
    "connectionProperties = {\n",
    "  \"user\" : \"tfo@az-tfo-serv-001.database.windows.net\",\n",
    "  \"password\" : \"Outlook@123\",\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "df = spark.read.jdbc(url=jdbcUrl, table=\"integration.filemaster\", properties=connectionProperties)\n",
    "#df.createOrReplaceTempView(\"sql_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02d2f5e-5236-4dbb-b27d-ed8530cd3a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|SourceUrl|     Table|\n+---------+----------+\n| circuits|EG_circuit|\n|    races|   EG_Race|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT SourceUrl,[Table] FROM integration.filemaster\"\n",
    "df = spark.read.jdbc(url=jdbcUrl, table=\"( \" + query + \" ) as filemaster\", properties=connectionProperties)\n",
    "\n",
    "# Show the retrieved data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f97850b-99e1-437c-8540-5f46b14ecbec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base URL for the Ergast API\n",
    "base_url = \"https://ergast.com/api/f1\"\n",
    "\n",
    "# Collect the distinct SourceUrl values from SQL (this could be a list of values)\n",
    "source_urls = [row['SourceUrl'] for row in df.collect()]\n",
    "table_names = [row['Table'] for row in df.collect()]\n",
    "\n",
    "# Prepare a list to store all the DataFrames (one for each SourceUrl)\n",
    "all_data = []\n",
    "\n",
    "# Loop through each SourceUrl and call the API\n",
    "for source_url in source_urls:\n",
    "    # Build the complete endpoint dynamically\n",
    "    year = \"2023\"\n",
    "    endpoint = f\"{base_url}/{year}/{source_url}.json\"\n",
    "               \n",
    "    \n",
    "    # Send a GET request to fetch the data\n",
    "    response = requests.get(endpoint)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Example for handling 'circuits' endpoint dynamically\n",
    "        # You need to dynamically normalize data depending on the endpoint\n",
    "        # Check if the endpoint corresponds to 'circuits' or another table\n",
    "        if source_url == \"circuits\":\n",
    "            data_normalized_circuits = pd.json_normalize(data['MRData']['CircuitTable']['Circuits'])\n",
    "        elif source_url == \"drivers\":\n",
    "            data_normalized_drivers = pd.json_normalize(data['MRData']['DriverTable']['Drivers'])\n",
    "        elif source_url == \"races\":\n",
    "            data_normalized_races = pd.json_normalize(data['MRData']['RaceTable']['Races'])    \n",
    "        else:\n",
    "            # Handle other tables if necessary\n",
    "            data_normalized_items = pd.json_normalize(data['MRData']['OtherTable']['Items'])\n",
    "        \n",
    "        # Append the normalized data to the list\n",
    "        #all_data.append(data_normalized)\n",
    "        \n",
    "        # Optional: Print or process the data\n",
    "        #print(f\"Data for {source_url}:\")\n",
    "        #print(data_normalized.head())\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {endpoint}: {response.status_code}\")\n",
    "\n",
    "# After processing all SourceUrls, you can combine all data into a single DataFrame\n",
    "#final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Show the final combined data\n",
    "#print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba6c5d8-605a-4d04-91d8-02030e4b42f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define catalog, schema (database), and table names\n",
    "catalog = \"projects\"\n",
    "schema = \"default\"\n",
    "#table_names = ['circuits', 'races']  # Assuming you have specific table names\n",
    "\n",
    "# Ensure data_normalized_circuits and data_normalized_races are Spark DataFrames\n",
    "data_normalized_circuits_spark = spark.createDataFrame(data_normalized_circuits)\n",
    "data_normalized_races_spark = spark.createDataFrame(data_normalized_races)\n",
    "\n",
    "# Iterate through each table name and load the corresponding data\n",
    "for tableName in table_names:\n",
    "    if tableName == \"EG_circuit\":\n",
    "        # Delete the existing table if it exists\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.{tableName}\")\n",
    "        # Create and store the table with the circuits data\n",
    "        data_normalized_circuits_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{tableName}\")\n",
    "    elif tableName == \"EG_Race\":\n",
    "        # Delete the existing table if it exists\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.{tableName}\")\n",
    "        # Create and store the table with the races data\n",
    "        data_normalized_races_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{tableName}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1401450172601428,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "API Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}